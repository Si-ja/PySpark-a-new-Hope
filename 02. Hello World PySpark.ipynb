{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lessons are further are continued from https://realpython.com/pyspark-intro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further is an equivalent to a Hello World in PySpark\n",
    "import pyspark\n",
    "# sc = pyspark.SparkContext(\"local[*]\")\n",
    "\n",
    "# If you have setup spark to run seperatelly, likely\n",
    "# your SparkContext has already started and running\n",
    "# You can check by uncommenting and running the following:\n",
    "\n",
    "# sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of lines and paragraphs in the PythonCopyright.txt is: 8\n"
     ]
    }
   ],
   "source": [
    "# An additional file has been added to this repo\n",
    "\n",
    "# In this case - amount of lines are calculated in the file.\n",
    "text_file = \"PythonCopyright.txt\"\n",
    "txt = sc.textFile(text_file)\n",
    "print(f\"The amount of lines and paragraphs in the {text_file} is: {txt.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of lines there are in which the word 'python' shows up in the text is: 1\n"
     ]
    }
   ],
   "source": [
    "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
    "print(f\"The amount of lines there are in which the word 'python' shows up in the text is: {python_lines.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you will want to properly work with `PySpark` you will have to refer from time to time to the following documenatations: [PySpark and API for it](http://spark.apache.org/docs/latest/api/python/index.html) and [Scala Documentation](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/api/index.html). The thing is, Spark is written with scala based on a functional language paradigm. With it, it is much easier to parallelize the executable code. Therefore, pretty much this approach is used in such cluster based calculations oriented systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use an example of RDD through parallelization\n",
    "big_list = range(100000)\n",
    "rdd = sc.parallelize(big_list, 2)\n",
    "odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "odds.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0033297538757324"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To time how long a process takes the following approach can be used\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "time.sleep(3)\n",
    "t1 = time.time()\n",
    "\n",
    "time_dif = t1 - t0\n",
    "time_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 2 distributed processes it took 3.77 seconds\n",
      "With 6 distributed processes it took 3.54 seconds\n",
      "With 10 distributed processes it took 2.75 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use the RDD process again but time it\n",
    "# And will save the whole output in memory\n",
    "bigger_list = range(10000000)\n",
    "\n",
    "# The difference will be in the slicing and amount of them\n",
    "import time\n",
    "dist = 2\n",
    "t0 = time.time()\n",
    "rdd_2 = sc.parallelize(bigger_list, dist)\n",
    "odds = rdd_2.filter(lambda x: x % 2 != 0)\n",
    "collector = odds.collect()\n",
    "t1 = time.time()\n",
    "time_dif = round(t1 - t0, 2)\n",
    "print(f\"With {dist} distributed processes it took {time_dif} seconds\")\n",
    "\n",
    "dist = 6\n",
    "t0 = time.time()\n",
    "rdd_8 = sc.parallelize(bigger_list, dist)\n",
    "odds = rdd_8.filter(lambda x: x % 2 != 0)\n",
    "collector = odds.collect()\n",
    "t1 = time.time()\n",
    "time_dif = round(t1 - t0, 2)\n",
    "print(f\"With {dist} distributed processes it took {time_dif} seconds\")\n",
    "\n",
    "dist = 10\n",
    "t0 = time.time()\n",
    "rdd_8 = sc.parallelize(bigger_list, dist)\n",
    "odds = rdd_8.filter(lambda x: x % 2 != 0)\n",
    "collector = odds.collect()\n",
    "t1 = time.time()\n",
    "time_dif = round(t1 - t0, 2)\n",
    "print(f\"With {dist} distributed processes it took {time_dif} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
